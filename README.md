# AI-Driven Cyber Threat Intelligence

A modular pipeline and dashboard for collecting, processing, classifying, and analyzing cyber vulnerability data from NVD and The Hacker News.

---

## ğŸš€ Project Overview

This project automates the ingestion, processing, and analysis of cybersecurity vulnerability data:

1. **Scraping**

   * Fetch CVE data from [NVD API](https://nvd.nist.gov) (incremental updates)
   * Scrape vulnerability articles from [The Hacker News](https://thehackernews.com)

2. **Preprocessing & Linking** (`clean_text.py`)

   * HTMLÂ â†’ text cleaning
   * Normalize and mask PII/CVE references
   * Extract CVSS bins, perform sentiment analysis, TFâ€‘IDF, and SBERT embeddings
   * Link articles â†” CVEs (count, earliest date)

3. **Threat Classification** (`threat_classifier.py`)

   * Multi-label classification into categories (Phishing, Malware, XSS, etc.)

4. **Urgency Scoring** (`urgency_scoring.py`)

   * Compute a composite urgency score from severity, sentiment, recency, and exploit/patch indicators
   * Discretize into Low/Medium/High levels

5. **Emerging Threat Detection** (`anomaly_detection.py`)

   * Zero-day keyword heuristic
   * Spike detection on mention counts
   * Isolation Forest on random-projected TFâ€‘IDF vectors

6. **Dashboard** (`dashboard/app.py`) (Work in Progress)

   * Streamlit UI for overview metrics, timelines, category distributions, urgency & emerging threats, and CVE details

7. **Automation**

   * **`scheduler.py`**: run scrapes every 6â€¯hours
   * **`run.py`**: CLI orchestrator for pipeline stages and dashboard

---

## ğŸ“¦ Prerequisites

* **PythonÂ 3.8+**
* **Virtual environment** (recommended)
* **NVD API key** in a `.env` file:

  ```bash
  NVD_API_KEY=YOUR_API_KEY_HERE
  ```

---

## âš™ï¸ Installation

```bash
git clone https://github.com/dgupta40/cyber-threat-intelligence.git
cd cyber-threat-intelligence
python -m venv .venv
source venv/bin/activate      # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

---

## ğŸš€ Usage

### 1. Run scheduler (periodic scraping)

```bash
python scheduler.py (Still to be implemented in the main pipline)
```

Runs an immediate scrape for NVD & Hacker News, then every 6â€¯hours.

### 2. Full pipeline & dashboard

```bash
# Run entire pipeline and launch dashboard
python run.py --component all --dashboard
```

Or run stages individually:

```bash
python run.py --component scrape       # Scrape data
python run.py --component preprocess  # Clean & link
python run.py --component categorize  # Threat classification
python run.py --component urgency     # Urgency scoring
python run.py --component detect_anomalies  # Emerging threats
python run.py --component dashboard   # Open Streamlit dashboard
```

---

## ğŸ—‚ï¸ Directory Structure

```
project-root/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # JSON from scrapers
â”‚   â””â”€â”€ processed/          # Parquet/CSV outputs
â”œâ”€â”€ dashboard/              # Streamlit app
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ scraper/                # Scraper modules
â”‚   â”œâ”€â”€ hackernews_scraper.py
â”‚   â””â”€â”€ nvd_scraper.py
â”œâ”€â”€ preprocessing/          # Cleaning & linking
â”‚   â””â”€â”€ clean_text.py
â”œâ”€â”€ pipeline/               # Analysis stages
â”‚   â”œâ”€â”€ threat_classifier.py
â”‚   â”œâ”€â”€ urgency_scoring.py
â”‚   â””â”€â”€ anomaly_detection.py
â”œâ”€â”€ utils/                  # Shared helpers
â”‚   â””â”€â”€ helpers.py
â”œâ”€â”€ scheduler.py            # APScheduler job definitions
â”œâ”€â”€ run.py                  # CLI orchestrator
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ¤ Contributing

1. Fork the repo
2. Create a feature branch (`git checkout -b feature/xyz`)
3. Commit changes (`git commit -m 'Add xyz'`)
4. Push to branch (`git push origin feature/xyz`)
5. Open a Pull Request